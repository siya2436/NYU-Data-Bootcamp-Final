# -*- coding: utf-8 -*-
"""Data Bootcamp Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSbmEWXQcuV8iTGi285-NrQFocRBQU9v
"""

import pandas as pd

csv_path = '/content/diabetes_012_health_indicators_BRFSS2015.xlsx'

df = pd.read_excel(csv_path)
print('Loaded', csv_path, 'shape =', df.shape)
df.head()

df['Diabetes_binary'] = (df['Diabetes_012'] != 0).astype(int)

"""### Part I: KNN Model Evaluation"""

feature_cols = [c for c in df.columns if c not in ['Diabetes_012', 'Diabetes_binary']]
X = df[feature_cols]
y = df['Diabetes_binary']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize the KNN model with n_neighbors=5 (you can tune this hyperparameter)
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model using the scaled training data
knn.fit(X_train_scaled, y_train)

# Make predictions on the scaled test data
y_pred = knn.predict(X_test_scaled)

# Evaluate the model performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix again (or use the existing one if available)
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for KNN Model')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

class_distribution = df['Diabetes_binary'].value_counts()
print("Class Distribution (0: No Diabetes, 1: Diabetes):")
display(class_distribution)

class_percentage = df['Diabetes_binary'].value_counts(normalize=True) * 100
print("\nClass Percentage:")
display(class_percentage)

# Visualize the class distribution
plt.figure(figsize=(6, 4))
sns.barplot(x=class_distribution.index, y=class_distribution.values, palette='viridis')
plt.title('Distribution of Diabetes_binary Classes')
plt.xlabel('Diabetes_binary (0: No, 1: Yes)')
plt.ylabel('Number of Samples')
plt.xticks([0, 1], ['No Diabetes (0)', 'Diabetes (1)'])
plt.show()

"""###Part II:
###Logistic Regression, XGBoost and Tabtransformer

"""

# Check Class Balance before training models
import matplotlib.pyplot as plt

target_col = "Diabetes_012"

# Value counts of the target: 0 = no diabetes, 1 = prediabetes, 2 = diabetes
class_counts = df[target_col].value_counts().sort_index()
print(class_counts)

# Simple bar plot of class distribution
plt.figure(figsize=(5,4))
class_counts.plot(kind="bar")
plt.xlabel("Diabetes_012 class")
plt.ylabel("Count")
plt.title("Class distribution of Diabetes_012")
plt.show()

df['Diabetes_binary'] = (df['Diabetes_012'] != 0).astype(int)

feature_cols = [c for c in df.columns if c not in ['Diabetes_012', 'Diabetes_binary']]
X = df[feature_cols]
y = df['Diabetes_binary']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

pip install xgboost pytorch-tabular

# Prepare for all models
from sklearn.model_selection import train_test_split
import numpy as np

# All feature columns are every column except the target
feature_cols = [c for c in df.columns if c != target_col]

# Make sure target is integer type
df[target_col] = df[target_col].astype("int64")

# (Optional) Make all features float for convenience
for col in feature_cols:
    df[col] = df[col].astype("float32")

# Split into train and test with stratification on the target
train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df[target_col],
)

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)

# Prepare NumPy arrays for sklearn / XGBoost models
X_train = train_df[feature_cols].values
y_train = train_df[target_col].values

X_test = test_df[feature_cols].values
y_test = test_df[target_col].values

# Standardize features (for Logistic Regression)
from sklearn.preprocessing import StandardScaler

# StandardScaler: subtract mean, divide by std
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""###1. Logistic Regression (baseline model)




"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Multinomial logistic regression for multi-class classification
log_reg = LogisticRegression(
    max_iter=1000,
    multi_class="multinomial"
)

# Fit on scaled training data
log_reg.fit(X_train_scaled, y_train)

# Predict class labels on test set
y_pred_lr = log_reg.predict(X_test_scaled)

# Compute accuracy and macro F1
acc_lr = accuracy_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr, average="macro")

print("Logistic Regression accuracy:", acc_lr)
print("Logistic Regression macro F1:", f1_lr)
print("\nClassification report:\n", classification_report(y_test, y_pred_lr))

# Confusion matrix visualization
cm_lr = confusion_matrix(y_test, y_pred_lr)
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=[0,1,2])

plt.figure(figsize=(4,4))
disp_lr.plot(values_format="d")
plt.title("Logistic Regression - Confusion Matrix")
plt.show()



"""###2. XGBoost model"""

from xgboost import XGBClassifier

# XGBoost classifier for multi-class
xgb = XGBClassifier(
    objective="multi:softprob",  # predict probability for each class
    num_class=3,
    eval_metric="mlogloss",
    tree_method="hist",          # fast histogram-based algorithm
    n_estimators=300,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    n_jobs=-1,
)

# Note: XGBoost can work well on unscaled data, so we use X_train directly
xgb.fit(X_train, y_train)

# Predict class labels on the test set
y_pred_xgb = xgb.predict(X_test)

# Compute accuracy and macro F1
acc_xgb = accuracy_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb, average="macro")

print("XGBoost accuracy:", acc_xgb)
print("XGBoost macro F1:", f1_xgb)
print("\nClassification report:\n", classification_report(y_test, y_pred_xgb))

# Confusion matrix visualization
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=[0,1,2])

plt.figure(figsize=(4,4))
disp_xgb.plot(values_format="d")
plt.title("XGBoost - Confusion Matrix")
plt.show()

"""###3. TabTransformer with pytorch-tabular

TabTransformer is a deep learning model designed for tabular data (data in rows and columns).
It works by:
Turning each feature (like BMI, smoking status, blood pressure) into a small learned representation.
Using Transformer attention blocks to learn how different features relate to each other.
Combining these learned relationships to make a prediction.

Why it fits?
Our dataset has many columns (health indicators), and some of them interact in meaningful ways. For example:BMI + exercise level, Blood pressure + age
Some models may treat these features independently, but TabTransformer can learn feature interactions automatically.

Treat some columns as continuous and others as categorical
"""

# Continuous columns: numeric values where magnitude matters
continuous_cols = ["BMI", "MentHlth", "PhysHlth"]

# The rest we treat as categorical (0/1 or small integers)
categorical_cols = [c for c in feature_cols if c not in continuous_cols]

# Set dtypes for pytorch-tabular:
# - categorical: 'category'
# - continuous: float
# Iterate through the identified categorical columns
for col in categorical_cols:
    # Convert the column to 'int64' first to handle potential float representations
    # Then convert to 'category' dtype, which is suitable for categorical features
    train_df[col] = train_df[col].astype("int64").astype("category")
    test_df[col] = test_df[col].astype("int64").astype("category")

# Iterate through the identified continuous columns
for col in continuous_cols:
    # Convert the column to 'float32' dtype, which is suitable for numerical features
    train_df[col] = train_df[col].astype("float32")
    test_df[col] = test_df[col].astype("float32")

# Rename the target column to 'target' for pytorch-tabular compatibility
# If the original target_col ('Diabetes_012') still exists, rename it.
if target_col in train_df.columns:
    train_df.rename(columns={target_col: 'target'}, inplace=True)
    test_df.rename(columns={target_col: 'target'}, inplace=True)

# Now, ensure that target_col_for_tabular refers to the 'target' column,
# assuming it has either just been renamed or was already named 'target'.
target_col_for_tabular = 'target'

# Ensure the target column is of 'int64' type for classification tasks
train_df[target_col_for_tabular] = train_df[target_col_for_tabular].astype("int64")
test_df[target_col_for_tabular] = test_df[target_col_for_tabular].astype("int64")

"""Configure and train TabTransformer"""

from pytorch_tabular import TabularModel
from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig
from pytorch_tabular.models import TabTransformerConfig
import torch
from omegaconf import DictConfig
from omegaconf.base import ContainerMetadata # Import ContainerMetadata
import typing # Import typing to access Any

# Add DictConfig and ContainerMetadata to safe globals to resolve UnpicklingError with PyTorch 2.6+
torch.serialization.add_safe_globals([DictConfig, ContainerMetadata, typing.Any])

# Describe the data for pytorch-tabular
data_config = DataConfig(
    target=[target_col_for_tabular],   # list with the name of the target column
    continuous_cols=continuous_cols,   # numeric features
    categorical_cols=categorical_cols, # categorical features
)

# Configure the TabTransformer model
model_config = TabTransformerConfig(
    task="classification",             # classification task
    metrics=["accuracy"],              # metric to monitor during training
    metrics_prob_input=[False]           # we provide logits internally
)

# Training configuration
trainer_config = TrainerConfig(
    max_epochs=10,     # increase if you want better performance
    batch_size=1024,
    progress_bar="simple",
    load_best=False  # Disable loading the best model to avoid UnpicklingError
)

optimizer_config = OptimizerConfig()   # default Adam optimizer

# Build the TabularModel
tab_model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    optimizer_config=optimizer_config,
    trainer_config=trainer_config,
)

# Train the TabTransformer
tab_model.fit(train=train_df, validation=test_df)

"""Predictions, accuracy for TabTransformer"""

# Predict on the test DataFrame
pred_df = tab_model.predict(test_df)

# Get the probability columns, assuming they follow 'target_X_probability' pattern
prob_cols = [f"target_{i}_probability" for i in range(len(pred_df.columns)) if f"target_{i}_probability" in pred_df.columns]

# Extract probabilities and find the class with the highest probability
y_pred_tab = pred_df[prob_cols].values.argmax(axis=1)
y_true_tab = test_df[target_col_for_tabular].values

# Compute accuracy and macro F1
acc_tab = accuracy_score(y_true_tab, y_pred_tab)
f1_tab = f1_score(y_true_tab, y_pred_tab, average="macro")

print("TabTransformer accuracy:", acc_tab)
print("TabTransformer macro F1:", f1_tab)
print("\nClassification report:\n", classification_report(y_true_tab, y_pred_tab))

# Confusion matrix visualization
cm_tab = confusion_matrix(y_true_tab, y_pred_tab)
disp_tab = ConfusionMatrixDisplay(confusion_matrix=cm_tab, display_labels=[0,1,2])

plt.figure(figsize=(4,4))
disp_tab.plot(values_format="d")
plt.title("TabTransformer - Confusion Matrix")
plt.show()



"""###4.Compare the three models"""

# Put all metrics into a small DataFrame
results = pd.DataFrame({
    "Model": ["Logistic Regression", "XGBoost", "TabTransformer"],
    "Accuracy": [acc_lr, acc_xgb, acc_tab],
    "Macro F1": [f1_lr, f1_xgb, f1_tab],
})

print(results)

# Plot accuracies as a bar chart
plt.figure(figsize=(6,4))
plt.bar(results["Model"], results["Accuracy"])
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.title("Model Accuracy Comparison")
plt.xticks(rotation=15)
plt.show()

"""###Conclusion

All three modelsâ€”Logistic Regression, XGBoost, and TabTransformerâ€”achieve very similar overall accuracy, each reaching approximately 98%. This high accuracy is expected because the dataset is imbalanced, with the majority of samples belonging to the non-diabetic class.
To better capture performance on both classes, the macro F1 score was also evaluated. Macro F1 measures how well the model performs on each class separately, which is especially important when one class (diabetes) is much smaller than the other. The macro F1 scores for all three models fall within a close range (0.64â€“0.65), indicating that:
All models perform similarly in distinguishing between diabetic and non-diabetic cases.
No single model significantly outperforms the others on balanced class-level performance.
Overall, the results suggest that although XGBoost and TabTransformer are more advanced models, Logistic Regression performs comparably well, likely because the dataset is relatively structured and the feature relationships are straightforward

Across all three modelsâ€”Logistic Regression, XGBoost, and TabTransformerâ€”the results show consistently high accuracy (around 98%). However, because the dataset is imbalanced, accuracy alone does not fully reflect how well each model identifies diabetic cases. To address this, macro F1 was used. Macro F1 measures how well the model performs on each class separately, which is especially important when one class (diabetes) is much smaller than the other. The similar macro F1 scores (around 0.64â€“0.65) indicate that all three models perform comparably in distinguishing between diabetic and non-diabetic individuals. Overall, despite differences in model complexity, the simpler Logistic Regression performs just as well as more advanced models for this dataset.

Across all three modelsâ€”Logistic Regression, XGBoost, and TabTransformerâ€”the results show consistently high accuracy (around 98%). However, because the dataset is imbalanced, accuracy alone does not fully reflect how well each model identifies diabetic cases. To address this, macro F1 was used. Macro F1 measures how well the model performs on each class separately, which is especially important when one class (diabetes) is much smaller than the other. The similar macro F1 scores (around 0.64â€“0.65) indicate that all three models perform comparably in distinguishing between diabetic and non-diabetic individuals. Overall, despite differences in model complexity, the simpler Logistic Regression performs just as well as more advanced models for this dataset.

###Part III:Multilayer Perceptron Model
"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""1. Multilayer Perceptron Model"""

# Define our mlp model with hidden_layer_sizes: (64, 32, 16)
mlp_v2 = MLPClassifier(
    hidden_layer_sizes=(64, 32, 16),
    max_iter=200,
    activation='relu',
    solver='adam',
    random_state=42,
    verbose=True,
    early_stopping=True,  # Enable early stopping; stops training if validation score doesn't improve
    n_iter_no_change=20   # Stop if validation score doesn't improve for 20 consecutive epochs
)

# Train the model
mlp_v2.fit(X_train_scaled, y_train)

# Make predictions on test set
y_pred_v2 = mlp_v2.predict(X_test_scaled)

#  Model evaluation
print("MLP V2 Model Evaluation:")

# Accuracy
accuracy_v2 = accuracy_score(y_test, y_pred_v2)
print(f"Accuracy: {accuracy_v2:.4f}")

# Classification report
print("\nClassification Report:")
# Since your dataset is highly imbalanced (non-diabetic cases far outnumber diabetic cases),
# we should pay more attention to F1-Score and Recall (especially for the 'Diabetes' class, i.e., class 1).
# Corrected: Provide target_names for all 3 classes (0, 1, 2)
print(classification_report(y_test, y_pred_v2, target_names=['No Diabetes (0)', 'Prediabetes (1)', 'Diabetes (2)']))


# Additional metrics for imbalanced classification
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score

# Calculate precision, recall, f1-score for each class
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_v2)

print("\nDetailed Metrics per Class:")
print("Class 0 (No Diabetes):")
print(f"  Precision: {precision[0]:.4f}")
print(f"  Recall: {recall[0]:.4f}")
print(f"  F1-Score: {f1[0]:.4f}")
print(f"  Support: {support[0]}")

print("\nClass 1 (Prediabetes):") # Corrected label
print(f"  Precision: {precision[1]:.4f}")
print(f"  Recall: {recall[1]:.4f}")
print(f"  F1-Score: {f1[1]:.4f}")
print(f"  Support: {support[1]}")

print("\nClass 2 (Diabetes):") # Added Class 2
print(f"  Precision: {precision[2]:.4f}")
print(f"  Recall: {recall[2]:.4f}")
print(f"  F1-Score: {f1[2]:.4f}")
print(f"  Support: {support[2]}")

# plot the confusion matrix
conf_mat_v2 = confusion_matrix(y_test, y_pred_v2)
plt.figure(figsize=(6, 5))
sns.heatmap(conf_mat_v2, annot=True, fmt='d', cmap='Blues')
plt.show()

"""**An optimization strategy by adjusting the classification decision threshold**"""

import numpy as np
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# # Get the probability predictions for each class from the MLP model on the test set
# probas[:, 1] represents the probability of being predicted as class 1 (Diabetes)
y_probas = mlp_v2.predict_proba(X_test_scaled)
probas_class_1 = y_probas[:, 1]

def evaluate_threshold(threshold, true_labels, probas_class_1):
    """Calculates prediction results and performance metrics based on a given threshold."""
    # If P(Diabetes=1) > threshold, predict as 1
    y_pred_tuned = (probas_class_1 > threshold).astype(int)

    # Calculate the confusion matrix
    cm = confusion_matrix(true_labels, y_pred_tuned)

    # TN, FP, FN, TP
    if cm.shape == (2, 2):
        TN, FP, FN, TP = cm.ravel()
    else:
        return None

    # Recall = TP / (TP + FN)
    recall_diabetes = TP / (TP + FN) if (TP + FN) > 0 else 0

    # Precision = TP / (TP + FP)
    precision_diabetes = TP / (TP + FP) if (TP + FP) > 0 else 0

    # Overall Accuracy
    accuracy = accuracy_score(true_labels, y_pred_tuned)

    return {
        'Threshold': threshold,
        'Accuracy': accuracy,
        'Recall (Diabetes)': recall_diabetes,
        'Precision (Diabetes)': precision_diabetes,
        'TP': TP, 'FN': FN, 'FP': FP
    }

thresholds = [0.5, 0.45, 0.40, 0.35, 0.30, 0.25]
results = []

for t in thresholds:
    result = evaluate_threshold(t, y_test, probas_class_1)
    if result:
        results.append(result)

# Print results
print("\n Performance Metrics at Different Thresholds")
for res in results:
    print(f"Threshold {res['Threshold']:.2f}: Acc={res['Accuracy']:.4f}, Recall={res['Recall (Diabetes)']:.4f}, Precision={res['Precision (Diabetes)']:.4f}, TP={res['TP']}, FN={res['FN']}")

# Selecting a threshold that increases recall 0.35
best_threshold = 0.35
y_pred_tuned = (probas_class_1 > best_threshold).astype(int)

# Create a binary version of y_test for evaluating 'Prediabetes' (class 1) vs 'Not Prediabetes' (classes 0 and 2)
y_test_binary_prediabetes = (y_test == 1).astype(int)

print(f"\nthe final classification report using the optimal threshold {best_threshold}")
print(classification_report(y_test_binary_prediabetes, y_pred_tuned, target_names=['Not Prediabetes', 'Prediabetes']))

"""The selection of the optimal threshold ( ðœƒ ) is a crucial cost-benefit trade-off, especially in high-risk classification scenarios like diabetes prediction. Our decision to choose  ðœƒ=0.35  was driven by balancing the need to prioritize Recall (minimizing missed diagnoses) against the need to maintain reasonable Precision (controlling over-diagnosis).

While adjusting the MLP's classification threshold ( ðœƒ=0.35 ) successfully increased the Diabetes class Recall from  21%  to  46% , there is still significant room for improvement in the model's performance

Potential Problem: The diabetes dataset exhibits significant Class Imbalance (the number of non-diabetes samples far outweighs the number of diabetes samples). Traditional neural networks, like the MLP, tend to optimize for overall accuracy during training, thus biasing the predictions towards the majority class. This inherent bias is the fundamental reason for the low Recall observed in the minority class (Diabetes).

Possible Solution: To systematically overcome this challenge, we may opt to transition to the Random Forest model. Random Forest is an ensemble learning method based on decision trees, and crucially, it supports a powerful built-in mechanism: class_weight='balanced'.By setting the key parameter class_weight='balanced', the Random Forest model automatically adjusts the weights of the minority class samples during training. This forces the model to incur a greater penalty for misclassifying a diabetes sample than for misclassifying a non-diabetes sample. This strategy effectively addresses the imbalance and allows the model to learn the characteristics of the minority class better, often yielding superior results, especially in terms of Recall, compared to the vanilla MLP.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Define our random forest model
# set class_weight='balanced' automatically balance unbalanced data
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    class_weight='balanced',    # Automatically increase the weights of a few categories
    random_state=42
)

# Training data
rf_model.fit(X_train_scaled, y_train)

# Evaluation
y_pred_rf = rf_model.predict(X_test_scaled)

print("\n-- Classification Report of Random Forest Model --")
print(classification_report(y_test, y_pred_rf, target_names=['No Diabetes (0)', 'Prediabetes (1)', 'Diabetes (2)']))

"""RF VS MLP"""

import pandas as pd

# 1. Define MLP Model Metrics (Threshold 0.35)
# Data extracted from your final classification report
mlp_metrics = {
    'Model': 'MLP (Threshold 0.35)',
    'Recall (Diabetes)': 0.46,
    'Precision (Diabetes)': 0.47,
    'F1-Score (Diabetes)': 0.46,
    'Accuracy': 0.83,
}

# 2. Define Random Forest Model Metrics (Balanced Weight)
# Data extracted from your Random Forest classification report
rf_metrics = {
    'Model': 'Random Forest (Balanced)',
    'Recall (Diabetes)': 0.76,
    'Precision (Diabetes)': 0.34,
    'F1-Score (Diabetes)': 0.47,
    'Accuracy': 0.73,
}

# 3. Create a Pandas DataFrame from the dictionaries
df_comparison = pd.DataFrame([mlp_metrics, rf_metrics])

# 4. Define the desired column order (for presentation)
column_order = [
    'Model',
    'Recall (Diabetes)',
    'Precision (Diabetes)',
    'F1-Score (Diabetes)',
    'Accuracy'
]

THRESHOLD = 0.5

y_pred_gc = (y_prob_test > THRESHOLD).astype(int)

precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_gc, average='binary')
accuracy = accuracy_score(y_test, y_pred_gc)

"""Probability Prediction-
***the Glivenko-Cantelli Theorem***

The core theoretical foundation of this study is the Glivenko-Cantelli Theorem, often regarded as the "Fundamental Theorem of Statistics." This theorem guarantees that given a sufficiently large sample size, the observed Empirical Distribution Function (ECDF) uniformly converges to the True Cumulative Distribution Function. Based on this axiom, I constructed a non-parametric probabilistic classifier. Unlike traditional machine learning approaches that often assume specific data distributions (e.g., Gaussian) or rely on opaque "black-box" mappings, this model directly utilizes training data to build empirical distributions for both "Diabetic" and "Healthy" populations across various features (such as BMI, HighBP, and Age). By calculating the divergence in probability density between these distributions - specifically the Weight of Evidence (WoE) - I aggregated the risk contributions of each feature as Log-Odds. This approach, grounded in Bayesian principles, yields a final probability prediction that is both mathematically rigorous and highly interpretable.

Glivenko-Cantelli Model offered a more stable balance between Precision (0.41) and Recall compared to the Random Forest, which sacrificed precision for recall. This suggests that by relying solely on distribution inference grounded in statistical axioms, we achieved predictive performance on par with complex machine learning models, while retaining the distinct advantage of being able to explicitly explain which features (e.g., BMI contributing +0.68 log-odds) drove the specific risk prediction.
"""

import numpy as np
import pandas as pd
from statsmodels.distributions.empirical_distribution import ECDF
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score

# 1. Define Core Model Class
class GlivenkoCantelliClassifier:
    def __init__(self):
        # Store empirical probability tables for each feature
        self.feature_distributions = {}
        self.base_log_odds = 0

    def fit(self, X, y):
        """
        Using GC Theorem: Directly use sample frequency as an estimate of true probability.
        """
        # Calculate base probability (Prior)
        p_sick = y.mean()
        self.base_log_odds = np.log(p_sick / (1 - p_sick))

        print("Building Empirical Distributions (Glivenko-Cantelli Approximation)...")

        # Iterate through every feature column
        for col in X.columns:
            # Separate data into Healthy and Sick groups
            sick_data = X.loc[y == 1, col]
            healthy_data = X.loc[y == 0, col]

            # Strategy: For continuous variables (like BMI), we need probability density.
            # The GC theorem gives CDF. To do Bayes prediction, we need P(x|Class).
            # Method: Bin the data (continuous -> discrete) and count frequencies.

            # Automatically decide whether to bin
            if X[col].nunique() > 20:
                # Continuous variable: Create bins (using pandas qcut for quantiles)
                # Use full dataset to define bin edges to ensure consistency
                _, bins = pd.qcut(X[col], q=10, retbins=True, duplicates='drop')

                # Count probabilities in each bin (Frequency â‰ˆ Probability by LLN/GC)
                sick_counts = pd.cut(sick_data, bins=bins).value_counts(normalize=True).sort_index()
                healthy_counts = pd.cut(healthy_data, bins=bins).value_counts(normalize=True).sort_index()

                # Store bin rules and probability tables
                self.feature_distributions[col] = {
                    'type': 'continuous',
                    'bins': bins,
                    'sick_probs': sick_counts,
                    'healthy_probs': healthy_counts
                }

            else:
                # Discrete variable (0/1, 1-5, etc.): Count frequencies directly
                # value_counts(normalize=True) is the probability mass function
                sick_probs = sick_data.value_counts(normalize=True)
                healthy_probs = healthy_data.value_counts(normalize=True)

                self.feature_distributions[col] = {
                    'type': 'discrete',
                    'sick_probs': sick_probs,
                    'healthy_probs': healthy_probs
                }

    def predict_proba(self, X):
        """
        Fixed version: Added .astype(float) to resolve TypeError with Categorical data
        """
        # Initialize scores with the base log-odds
        log_odds = np.full(len(X), self.base_log_odds)

        # Superimpose scores feature by feature
        for col in X.columns:
            if col not in self.feature_distributions:
                continue

            info = self.feature_distributions[col]
            eps = 1e-6

            if info['type'] == 'discrete':
                # Map value to probability, force float, fill NaNs
                p_sick = X[col].map(info['sick_probs']).astype(float).fillna(eps)
                p_healthy = X[col].map(info['healthy_probs']).astype(float).fillna(eps)

            else: # continuous
                bins = info['bins']
                # pandas cut returns Categorical objects
                cats = pd.cut(X[col], bins=bins)

                # Map category to probability, force float, fill NaNs
                p_sick = cats.map(info['sick_probs']).astype(float).fillna(eps)
                p_healthy = cats.map(info['healthy_probs']).astype(float).fillna(eps)

            # Calculate Weight of Evidence (WoE)
            woe = np.log(p_sick / p_healthy)
            log_odds += woe

        # Convert Log-Odds back to 0-1 Probability (Sigmoid)
        return 1 / (1 + np.exp(-log_odds))

# 2. Execution / Training


# Assuming df, X, y are already prepared
# feature_cols = [c for c in df.columns if c not in ['Diabetes_012', 'Diabetes_binary']]
# X = df[feature_cols]
# y = df['Diabetes_binary']

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instantiate and Fit
gc_model = GlivenkoCantelliClassifier()
gc_model.fit(X_train, y_train)

# Predict on Test Set
y_prob_test = gc_model.predict_proba(X_test)

# Evaluate
auc = roc_auc_score(y_test, y_prob_test)
print(f"\nModel Evaluation Results:")
print(f"AUC Score: {auc:.4f}")


# 3. Helper: Create Default Profile for Missing Features
# This ensures every feature is considered even if you only input a few.
all_features = X_train.columns.tolist()
default_patient_profile = {}
for col in all_features:
    if X_train[col].nunique() > 20:
        default_patient_profile[col] = X_train[col].median()
    else:
        default_patient_profile[col] = X_train[col].mode()[0]

# 4. Predict Single Person (English Output & Full Feature Support)
def predict_full_profile(custom_values=None):
    if custom_values is None:
        custom_values = {}

    # 1. Start with the default "average" profile
    patient_data = default_patient_profile.copy()

    # 2. Update with user provided values
    for key, val in custom_values.items():
        if key in patient_data:
            patient_data[key] = val

    # 3. Convert to DataFrame
    patient_df = pd.DataFrame([patient_data])[all_features]

    # 4. Predict
    probability = gc_model.predict_proba(patient_df)[0]
    base_score = gc_model.base_log_odds

    # 5. Print English Report
    print(f"\n=== Full Feature Prediction Report ===")
    print(f"Base Score (Log-Odds): {base_score:.2f}")

    # Analyze contributions
    print("-" * 60)
    print(f"{'Feature':<20} | {'Value':<8} | {'Impact (WoE)':<15} | {'Dist (Sick vs Healthy)'}")
    print("-" * 60)

    for col, val in custom_values.items():
        if col not in gc_model.feature_distributions: continue
        info = gc_model.feature_distributions[col]
        eps = 1e-6

        # Retrieve probabilities for display
        if info['type'] == 'discrete':
            p_s = info['sick_probs'].get(val, eps)
            p_h = info['healthy_probs'].get(val, eps)
        else:
            # Simplified lookup for display purposes
            bins = info['bins']
            # Find which bin this value falls into
            try:
                bin_idx = np.digitize([val], bins)[0] - 1
                bin_idx = max(0, min(bin_idx, len(bins)-2))
                p_s = info['sick_probs'].iloc[bin_idx] if bin_idx < len(info['sick_probs']) else eps
                p_h = info['healthy_probs'].iloc[bin_idx] if bin_idx < len(info['healthy_probs']) else eps
            except:
                p_s, p_h = eps, eps

        woe = np.log(p_s / p_h)
        impact = "Increases Risk" if woe > 0 else "Decreases Risk"

        print(f"{col:<20} | {str(val):<8} | {woe:+.2f} ({impact}) | {p_s:.0%} vs {p_h:.0%}")

    print("-" * 60)
    print(f"Final Predicted Probability: {probability:.2%}")

# Test Case
sample_patient = {
    'HighBP': 1.0,
    'HighChol': 1.0,
    'BMI': 35.0,
    'Smoker': 1.0,
    'Age': 10.0
}

predict_full_profile(sample_patient)

"""The Glivenko-Cantelli Classifier demonstrates good discriminative ability with an AUC Score of 0.81. Its strength lies in interpretability, clearly showing how each feature (e.g., HighBP +0.69, BMI +0.68) contributes to the final predicted probability for an individual patient (e.g., 14.30% for the sample patient). This is achieved through Weight of Evidence (WoE), which quantifies feature impact by comparing probability distributions in 'sick' vs. 'healthy' groups. This model provides transparent, statistically grounded probability predictions, crucial for understanding 'why' a prediction is made, beyond just 'what' the prediction is.

###**Project Summary and Model Performance Insights**
"""

import pandas as pd

# Gather metrics for all models from the latest executed outputs
all_model_metrics = [
    {
        'Model': 'KNN (Diabetes_binary)',
        'Accuracy': 0.830554,  # from 9ed3a61e
        'Precision (Class 1)': 0.43,
        'Recall (Class 1)': 0.24,
        'F1-Score (Class 1)': 0.31,
    },
    {
        'Model': 'Logistic Regression (Prediabetes)', # Class 1 from Diabetes_012
        'Accuracy': 0.981749,  # from PzJDAq8gxh0M
        'Precision (Class 1)': 0.00,
        'Recall (Class 1)': 0.00,
        'F1-Score (Class 1)': 0.00,
    },
    {
        'Model': 'XGBoost (Prediabetes)', # Class 1 from Diabetes_012
        'Accuracy': 0.981630,  # from XO8eBeBqxh61
        'Precision (Class 1)': 0.29,
        'Recall (Class 1)': 0.00,
        'F1-Score (Class 1)': 0.01,
    },
    {
        'Model': 'TabTransformer (Prediabetes)', # Class 1 from Diabetes_012
        'Accuracy': 0.981768,  # from 1Yf8JQ_JsQwa
        'Precision (Class 1)': 1.00, # Note: Precision can be 1.0 if no true positives are predicted, and no false positives are made.
        'Recall (Class 1)': 0.00,
        'F1-Score (Class 1)': 0.00,
    },
    {
        'Model': 'MLP (Prediabetes, Thr=0.35)', # From QnS0gjZsv5Fn (binary for Prediabetes)
        'Accuracy': 0.98, # Note: This accuracy is for the binarized problem, not the 3-class problem.
        'Precision (Class 1)': 0.75,
        'Recall (Class 1)': 0.00,
        'F1-Score (Class 1)': 0.01,
    },
    {
        'Model': 'Random Forest (Prediabetes, Balanced)', # Class 1 from Diabetes_012, balanced weights
        'Accuracy': 0.95, # from zhRivGzEwJg1
        'Precision (Class 1)': 0.17,
        'Recall (Class 1)': 0.39,
        'F1-Score (Class 1)': 0.23,
    },
    {
        'Model': 'GC Classifier (Diabetes_binary)', # From _2wrmY6U72lq
        'Accuracy': 0.81,
        'Precision (Class 1)': 0.41,
        'Recall (Class 1)': 0.52,
        'F1-Score (Class 1)': 0.46,
    }
]

df_comparison_full = pd.DataFrame(all_model_metrics)

# Define the desired column order for presentation
column_order_full = [
    'Model',
    'Accuracy',
    'Precision (Class 1)',
    'Recall (Class 1)',
    'F1-Score (Class 1)',
]

print("\n--- Comparison between all the models (Class 1) ---")
print(df_comparison_full[column_order_full].to_markdown(index=False, floatfmt=".2f"))

"""### Key Findings

*   **Misleading Overall Accuracy**: Although multiple models (Logistic Regression, XGBoost, **TabTransformer**, MLP) achieved very similar overall accuracies, reaching approximately 98%, they almost completely failed to identify the minority class (Prediabetes), with recall close to 0%. This highlights that overall accuracy can be a misleading metric when dealing with highly imbalanced datasets.

*   **Challenges in Minority Class Identification**:
    *   The **KNN model** showed limited ability to identify the minority class, with a recall of 24% and an F1-Score of 31%.
    *   **Logistic Regression, XGBoost, and TabTransformer** all showed high overall accuracy (0.98), but **failed to identify any of the Prediabetes (Class 1) cases**, resulting in 0.00% Recall and 0.00% F1-Score for this class. TabTransformer, in particular, showed a Precision of 1.00 for Class 1, which implies it made no false positive predictions for this class, but also no true positive predictions.
    *   The **MLP model** (even with threshold adjustment) also struggled significantly with the Prediabetes class, achieving 0.00% Recall.
    *   **Random Forest (Balanced)**, through class weighting, significantly improved the recall for the minority class to 39%, but at the cost of precision (17%).
    *   The **Glivenko-Cantelli (GC) Classifier** demonstrated the most balanced performance across all models, achieving the highest recall for the minority class (52%), an F1-Score of 46%, and a precision of 41%. Despite having the lowest overall accuracy (81%), it performed best in identifying the minority class.

### Key Insights and Future Directions

*   **Medical Application Priority**: In medical scenarios like diabetes prediction, **reducing false negatives (high recall) is typically more crucial than achieving high overall accuracy**, even if it means a higher rate of false positives.

*   **Addressing Class Imbalance is Key**: The analysis emphasizes the necessity of employing specialized techniques to handle class imbalance. Compared to standard models optimized for overall accuracy, **models explicitly designed to address imbalanced data, such as Random Forest (Balanced) and the Glivenko-Cantelli Classifier, performed better in minority class identification**.

*   **Model Selection Trade-offs**:
    *   For scenarios requiring high interpretability and strict recall for minority classes, the **GC Classifier** is the optimal choice.
    *   For scenarios needing higher recall and accepting lower precision, **Random Forest (Balanced)** offers a practical solution.

Future model development should continue to explore more advanced techniques for handling class imbalance to further enhance the comprehensive performance and robustness of models, particularly in ensuring the accurate identification of minority classes.
"""